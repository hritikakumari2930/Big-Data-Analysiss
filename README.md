# Big-Data-Analysiss
*Company*: CODTECH IT SOLUTIONS
*Name* : Hritika Kumari
*Intern ID* : CT04DG1819
*Domain* : DATA ANALYTICS
*Duration* : 4 weeks
*Mentor* : NEELA SANTOSH

DESCRIPTION :-
Thanks, Hritika! Based on your uploaded notebook `Welcome_To_Colab.ipynb`, here’s a **short description** of what happened in the project and how everything worked — written in simple terms for GitHub or reports:

---

## ⚙️ What Was Done in This Project (Summary)

In this project, I used **PySpark** on **Google Colab** to analyze a large dataset. The steps I followed are:

1. ✅ **Set up PySpark in Google Colab**

   * Installed Java and Spark
   * Configured environment variables
   * Installed `findspark` to use PySpark in Colab

2. ✅ **Started a Spark Session**

   * Created a Spark session using `SparkSession.builder.appName()`
   * Verified Spark is running properly

3. ✅ **Loaded the Dataset**

   * Used `spark.read.csv()` to read a CSV file
   * Enabled header and schema detection

4. ✅ **Explored the Data**

   * Displayed top rows using `.show()`
   * Viewed column names and structure with `.columns` and `.printSchema()`

5. ✅ **Checked for Missing Values**

   * Used PySpark functions to count null values in each column
   * Helpful for cleaning and preparing the data

6. ✅ **Basic Analysis**

   * Performed basic filtering and transformation
   * Example: filtering specific rows or analyzing column data

---

##  How It Worked

PySpark was used to handle large-scale data processing. Google Colab provided a cloud environment to run Spark without setup on a local machine. Spark DataFrames allowed me to process data efficiently, even if it's large, by using distributed computing.

This project taught me how to clean, explore, and analyze big data step-by-step using PySpark in a real-world scenario.

---
